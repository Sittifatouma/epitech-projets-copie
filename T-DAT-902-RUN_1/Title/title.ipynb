{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5efc888",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Conv2D, MaxPooling2D, Flatten, Dense\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "data_folder = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/Multi_Label_dataset/Images\"\n",
    "poster_filenames = os.listdir(data_folder)\n",
    "metadata_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/IMDb movies.csv\"\n",
    "metadata = pd.read_csv(metadata_filename)\n",
    "\n",
    "# Prétraitement des données\n",
    "poster_size = (128, 128)\n",
    "num_classes = len(metadata[\"genre\"].unique())\n",
    "genre_list = list(metadata[\"genre\"].unique())\n",
    "X = np.empty((len(poster_filenames), poster_size[0], poster_size[1], 3), dtype=np.uint8)\n",
    "y = np.zeros((len(poster_filenames), num_classes), dtype=np.uint8)\n",
    "for i, filename in enumerate(poster_filenames):\n",
    "    img = Image.open(os.path.join(data_folder, filename)).convert(\"RGB\")\n",
    "    img = img.resize(poster_size)\n",
    "    X[i] = np.array(img)\n",
    "    imdb_id = os.path.splitext(filename)[0]\n",
    "    genre = metadata[metadata[\"imdb_title_id\"] == imdb_id][\"genre\"]\n",
    "    for g in genre:\n",
    "        y[i, genre_list.index(g)] = 1\n",
    "\n",
    "\n",
    "# Création du modèle CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(poster_size[0], poster_size[1], 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X)\n",
    "predicted_genre_indices = np.argmax(y_pred, axis=1)\n",
    "predicted_genres = [genre_list[i] for i in predicted_genre_indices]\n",
    "true_genre_indices = np.argmax(y, axis=1)\n",
    "true_genres = [genre_list[i] for i in true_genre_indices]\n",
    "print(\"Classification Report:\\n\", classification_report(true_genres, predicted_genres))\n",
    "\n",
    "# Prédiction sur un poster donné\n",
    "poster_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/outer.jpg\"\n",
    "img = Image.open(poster_filename).convert(\"RGB\")\n",
    "img = img.resize(poster_size)\n",
    "X_pred = np.array(img).reshape(1, poster_size[0], poster_size[1], 3)\n",
    "y_pred = model.predict(X_pred)\n",
    "predicted_genre = genre_list[np.argmax(y_pred)]\n",
    "print(\"Predicted genre:\", predicted_genre)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fcb91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/Multi_Label_dataset/Images\"\n",
    "poster_filenames = os.listdir(data_folder)\n",
    "metadata_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/IMDb movies.csv\"\n",
    "metadata = pd.read_csv(metadata_filename)\n",
    "\n",
    "# Prétraitement des données\n",
    "poster_size = (128, 128)\n",
    "num_classes = len(metadata[\"genre\"].unique())\n",
    "genre_list = list(metadata[\"genre\"].unique())\n",
    "X = np.empty((len(poster_filenames), poster_size[0], poster_size[1], 3), dtype=np.uint8)\n",
    "y = np.zeros((len(poster_filenames), num_classes), dtype=np.uint8)\n",
    "for i, filename in enumerate(poster_filenames):\n",
    "    img = Image.open(os.path.join(data_folder, filename)).convert(\"RGB\")\n",
    "    img = img.resize(poster_size)\n",
    "    X[i] = np.array(img)\n",
    "    imdb_id = os.path.splitext(filename)[0]\n",
    "    genre = metadata[metadata[\"imdb_title_id\"] == imdb_id][\"genre\"]\n",
    "    for g in genre:\n",
    "        y[i, genre_list.index(g)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Création du modèle CNN avec régularisation L2\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.01), input_shape=(poster_size[0], poster_size[1], 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6531ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X)\n",
    "predicted_genre_indices = np.argmax(y_pred, axis=1)\n",
    "predicted_genres = [genre_list[i] for i in predicted_genre_indices]\n",
    "true_genre_indices = np.argmax(y, axis=1)\n",
    "true_genres = [genre_list[i] for i in true_genre_indices]\n",
    "print(\"Classification Report:\\n\", classification_report(true_genres, predicted_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X)\n",
    "predicted_genre_indices = np.argmax(y_pred, axis=1)\n",
    "predicted_genres = [genre_list[i] for i in predicted_genre_indices]\n",
    "true_genre_indices = np.argmax(y, axis=1)\n",
    "true_genres = [genre_list[i] for i in true_genre_indices]\n",
    "\n",
    "# Rapport de classification\n",
    "report = classification_report(true_genres, predicted_genres, output_dict=True)\n",
    "\n",
    "# Création du graphique\n",
    "precision = []\n",
    "recall = []\n",
    "f1_score = []\n",
    "for genre in genre_list:\n",
    "    if genre in report:\n",
    "        precision.append(report[genre]['precision'])\n",
    "        recall.append(report[genre]['recall'])\n",
    "        f1_score.append(report[genre]['f1-score'])\n",
    "    else:\n",
    "        precision.append(0)\n",
    "        recall.append(0)\n",
    "        f1_score.append(0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(len(genre_list))\n",
    "bar_width = 0.3\n",
    "opacity = 0.8\n",
    "\n",
    "rects1 = ax.bar(index, precision, bar_width,\n",
    "                alpha=opacity, color='b',\n",
    "                label='Precision')\n",
    "\n",
    "rects2 = ax.bar(index + bar_width, recall, bar_width,\n",
    "                alpha=opacity, color='g',\n",
    "                label='Recall')\n",
    "\n",
    "rects3 = ax.bar(index + 2*bar_width, f1_score, bar_width,\n",
    "                alpha=opacity, color='r',\n",
    "                label='F1-score')\n",
    "\n",
    "ax.set_xlabel('Genres')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Classification Report')\n",
    "ax.set_xticks(index + bar_width)\n",
    "ax.set_xticklabels(genre_list, rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction sur un poster donné\n",
    "poster_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "img = Image.open(poster_filename).convert(\"RGB\")\n",
    "img = img.resize(poster_size)\n",
    "X_pred = np.array(img).reshape(1, poster_size[0], poster_size[1], 3)\n",
    "y_pred = model.predict(X_pred)\n",
    "predicted_genre = genre_list[np.argmax(y_pred)]\n",
    "print(\"Predicted genre:\", predicted_genre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(true_genres, predicted_genres, output_dict=True)\n",
    "genre_precision = {}\n",
    "for genre in genre_list:\n",
    "    genre_precision[genre] = report[genre]['precision']\n",
    "    \n",
    "genre_with_highest_precision = max(genre_precision, key=genre_precision.get)\n",
    "print(\"Genre with highest precision:\", genre_with_highest_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_precision = {}\n",
    "for genre in genre_list:\n",
    "    genre_precision[genre] = report.get(genre, {'precision': 0})['precision']\n",
    "\n",
    "genre_with_highest_precision = max(genre_precision, key=genre_precision.get)\n",
    "print(\"Genre with highest precision:\", genre_with_highest_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe37524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90790f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/Multi_Label_dataset/Images\"\n",
    "poster_filenames = os.listdir(data_folder)\n",
    "metadata_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/IMDb movies.csv\"\n",
    "metadata = pd.read_csv(metadata_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données\n",
    "poster_size = (128, 128, 1)  # Modification de la taille pour inclure l'image en niveaux de gris\n",
    "num_classes = len(metadata[\"genre\"].unique())\n",
    "genre_list = list(metadata[\"genre\"].unique())\n",
    "X = np.empty((len(poster_filenames), poster_size[0], poster_size[1], poster_size[2]), dtype=np.uint8)  # Modification de la dimension\n",
    "y = np.zeros((len(poster_filenames), num_classes), dtype=np.uint8)\n",
    "for i, filename in enumerate(poster_filenames):\n",
    "    img = Image.open(os.path.join(data_folder, filename)).convert(\"L\")  # Modification de la conversion en niveaux de gris\n",
    "    img = img.resize((poster_size[0], poster_size[1]))  # Redimensionnement\n",
    "    X[i] = np.array(img).reshape(poster_size)  # Modification pour inclure la dimension supplémentaire\n",
    "    imdb_id = os.path.splitext(filename)[0]\n",
    "    genre = metadata[metadata[\"imdb_title_id\"] == imdb_id][\"genre\"]\n",
    "    for g in genre:\n",
    "        y[i, genre_list.index(g)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(poster_size[0], poster_size[1], poster_size[2])))  # Modification de la valeur de input_shape\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec107e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8713fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X)\n",
    "predicted_genre_indices = np.argmax(y_pred, axis=1)\n",
    "predicted_genres = [genre_list[i] for i in predicted_genre_indices]\n",
    "true_genre_indices = np.argmax(y, axis=1)\n",
    "true_genres = [genre_list[i] for i in true_genre_indices]\n",
    "print(\"Classification Report:\\n\", classification_report(true_genres, predicted_genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f11c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Prédiction sur un poster donné\n",
    "poster_filename = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "img = Image.open(poster_filename).convert(\"L\")  # Conversion en niveaux de gris\n",
    "img = img.resize((poster_size[0], poster_size[1]))  # Redimensionnement\n",
    "X_pred = np.array(img).reshape(1, poster_size[0], poster_size[1], poster_size[2])  # Modification du nombre de canaux\n",
    "y_pred = model.predict(X_pred)\n",
    "predicted_genre_indices = np.argmax(y_pred, axis=1)\n",
    "predicted_genres = [genre_list[i] for i in predicted_genre_indices]\n",
    "predicted_genre = predicted_genres[0]\n",
    "print(\"Predicted genre:\", predicted_genre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def extraire_couleur_predominante(image_path):\n",
    "    # Charger l'image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convertir l'image en mode RGB\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # Obtenir les dimensions de l'image\n",
    "    largeur, hauteur = image.size\n",
    "\n",
    "    # Créer un histogramme des couleurs\n",
    "    histogramme = image.histogram()\n",
    "\n",
    "    # Trouver l'indice de la valeur maximale dans l'histogramme\n",
    "    couleur_predominante_index = max(enumerate(histogramme), key=lambda x: x[1])[0]\n",
    "\n",
    "    # Convertir l'indice en valeurs RGB\n",
    "    couleur_predominante = (\n",
    "        couleur_predominante_index // (256 * 256),  # Rouge\n",
    "        (couleur_predominante_index // 256) % 256,  # Vert\n",
    "        couleur_predominante_index % 256           # Bleu\n",
    "    )\n",
    "\n",
    "    return couleur_predominante\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "couleur = extraire_couleur_predominante(image_path)\n",
    "print(\"Couleur prédominante : RGB\", couleur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def extraire_couleur_predominante(image_path):\n",
    "    # Charger l'image\n",
    "    image = io.imread(image_path)\n",
    "\n",
    "    # Redimensionner l'image si nécessaire\n",
    "    # image = io.resize(image, (nouvelle_largeur, nouvelle_hauteur))\n",
    "\n",
    "    # Aplatir l'image en un tableau 2D\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    # Utiliser K-means pour regrouper les couleurs\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Obtenir les centres des clusters\n",
    "    couleurs_centrales = kmeans.cluster_centers_\n",
    "\n",
    "    # Convertir les valeurs RGB en entiers\n",
    "    couleur_predominante = tuple(map(int, couleurs_centrales[0]))\n",
    "\n",
    "    return couleur_predominante\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "couleur = extraire_couleur_predominante(image_path)\n",
    "print(\"Couleur prédominante : RGB\", couleur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def extraire_couleur_predominante(image_path):\n",
    "     # Charger l'image\n",
    "    image = io.imread(image_path)\n",
    "\n",
    "    # Redimensionner l'image si nécessaire\n",
    "    # image = io.resize(image, (nouvelle_largeur, nouvelle_hauteur))\n",
    "\n",
    "    # Aplatir l'image en un tableau 2D\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    # Utiliser K-means pour regrouper les couleurs\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Obtenir les centres des clusters\n",
    "    couleurs_centrales = kmeans.cluster_centers_\n",
    "\n",
    "    # Convertir les valeurs RGB en entiers\n",
    "    couleur_predominante = tuple(map(int, couleurs_centrales[0]))\n",
    "\n",
    "    return couleur_predominante\n",
    "\n",
    "# Chemin du dossier contenant les images\n",
    "dossier_images = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/Multi_Label_dataset/Images\"\n",
    "\n",
    "# Chemin du fichier CSV de sortie\n",
    "chemin_csv = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/csv\"\n",
    "\n",
    "# Liste pour stocker les données des images\n",
    "donnees_images = []\n",
    "\n",
    "# Parcourir les fichiers du dossier\n",
    "for nom_fichier in os.listdir(dossier_images):\n",
    "    chemin_image = os.path.join(dossier_images, nom_fichier)\n",
    "\n",
    "    # Vérifier si le fichier est une image\n",
    "    if os.path.isfile(chemin_image) and nom_fichier.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        couleur_predominante = extraire_couleur_predominante(chemin_image)\n",
    "        donnees_images.append((nom_fichier, couleur_predominante))\n",
    "\n",
    "# Écrire les données dans le fichier CSV\n",
    "with open(chemin_csv, 'w', newline='') as fichier_csv:\n",
    "    writer = csv.writer(fichier_csv)\n",
    "    writer.writerow(['Nom du fichier', 'Couleur prédominante (R,G,B)'])\n",
    "    for donnee_image in donnees_images:\n",
    "        writer.writerow(donnee_image)\n",
    "\n",
    "print(\"Fichier CSV créé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from skimage import io\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def extraire_couleur_predominante(image_path):\n",
    "    # Charger l'image\n",
    "    image = io.imread(image_path)\n",
    "\n",
    "    # Aplatir l'image en un tableau 2D\n",
    "    pixels = image.reshape(-1, 3)\n",
    "\n",
    "    # Utiliser K-means pour regrouper les couleurs\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "    kmeans.fit(pixels)\n",
    "\n",
    "    # Obtenir les centres des clusters\n",
    "    couleurs_centrales = kmeans.cluster_centers_\n",
    "\n",
    "    # Convertir les valeurs RGB en entiers\n",
    "    couleur_predominante = tuple(map(int, couleurs_centrales[0]))\n",
    "\n",
    "    return couleur_predominante\n",
    "\n",
    "# Chemin du dossier contenant les images\n",
    "dossier_images = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/data/Multi_Label_dataset/Images\"\n",
    "\n",
    "# Chemin du fichier CSV de sortie\n",
    "chemin_csv = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/csv\"\n",
    "\n",
    "# Liste pour stocker les données des images\n",
    "donnees_images = []\n",
    "\n",
    "# Parcourir récursivement les sous-dossiers et fichiers\n",
    "for dossier_racine, sous_dossiers, fichiers in os.walk(dossier_images):\n",
    "    for nom_fichier in fichiers:\n",
    "        chemin_image = os.path.join(dossier_racine, nom_fichier)\n",
    "\n",
    "        # Vérifier si le fichier est une image\n",
    "        if nom_fichier.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            couleur_predominante = extraire_couleur_predominante(chemin_image)\n",
    "            donnees_images.append((chemin_image, couleur_predominante))\n",
    "\n",
    "# Écrire les données dans le fichier CSV\n",
    "with open(chemin_csv, 'w', newline='') as fichier_csv:\n",
    "    writer = csv.writer(fichier_csv)\n",
    "    writer.writerow(['Chemin de l\\'image', 'Couleur prédominante (R,G,B)'])\n",
    "    for donnee_image in donnees_images:\n",
    "        writer.writerow(donnee_image)\n",
    "\n",
    "print(\"Fichier CSV créé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c74901",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image_to_string() got an unexpected keyword argument 'boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[1;32m     41\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m titre, position_titre \u001b[38;5;241m=\u001b[39m \u001b[43mdetecter_titre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitre:\u001b[39m\u001b[38;5;124m\"\u001b[39m, titre)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition du titre:\u001b[39m\u001b[38;5;124m\"\u001b[39m, position_titre)\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mdetecter_titre\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m texte \u001b[38;5;241m=\u001b[39m image_to_string(image_gris)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extraire les coordonnées de la zone de texte détectée\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m boite_texte \u001b[38;5;241m=\u001b[39m \u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_gris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m boite_texte \u001b[38;5;241m=\u001b[39m boite_texte\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculer la position verticale moyenne de la boîte de texte détectée\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: image_to_string() got an unexpected keyword argument 'boxes'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pytesseract import image_to_string\n",
    "\n",
    "def detecter_titre(image_path):\n",
    "    # Charger l'image avec OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris pour faciliter la détection des caractères\n",
    "    image_gris = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Utiliser pytesseract-mac pour effectuer l'OCR sur l'image en niveaux de gris\n",
    "    texte = image_to_string(image_gris)\n",
    "\n",
    "    # Extraire les coordonnées de la zone de texte détectée\n",
    "    boite_texte = image_to_string(image_gris, boxes=True)\n",
    "    boite_texte = boite_texte.splitlines()\n",
    "\n",
    "    # Calculer la position verticale moyenne de la boîte de texte détectée\n",
    "    positions_verticales = [int(line.split(' ')[1]) for line in boite_texte]\n",
    "    position_moyenne = sum(positions_verticales) / len(positions_verticales)\n",
    "\n",
    "    # Déterminer la position du titre dans l'image\n",
    "    hauteur_image = image.shape[0]\n",
    "    position_relative = position_moyenne / hauteur_image\n",
    "\n",
    "    # Déterminer si la position est en haut, au milieu ou en bas\n",
    "    seuil_haut = 0.3\n",
    "    seuil_bas = 0.7\n",
    "\n",
    "    if position_relative < seuil_haut:\n",
    "        position = \"haut\"\n",
    "    elif position_relative < seuil_bas:\n",
    "        position = \"milieu\"\n",
    "    else:\n",
    "        position = \"bas\"\n",
    "\n",
    "    return texte, position\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "titre, position_titre = detecter_titre(image_path)\n",
    "print(\"Titre:\", titre)\n",
    "print(\"Position du titre:\", position_titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e6935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyocr\n",
    "from PIL import Image\n",
    "\n",
    "def detecter_titre(image_path):\n",
    "    # Charger l'image avec OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris pour faciliter la détection des caractères\n",
    "    image_gris = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialiser l'outil OCR avec PyOCR\n",
    "    outils_ocr = pyocr.get_available_tools()\n",
    "    if len(outils_ocr) == 0:\n",
    "        raise Exception(\"Aucun outil OCR disponible\")\n",
    "    ocr_tool = outils_ocr[0]\n",
    "\n",
    "    # Utiliser l'outil OCR pour effectuer l'OCR sur l'image en niveaux de gris\n",
    "    texte = ocr_tool.image_to_string(Image.fromarray(image_gris))\n",
    "\n",
    "    # ... effectuer d'autres opérations de traitement du texte ...\n",
    "\n",
    "    return texte\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_path = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "titre = detecter_titre(image_path)\n",
    "print(\"Titre:\", titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eba24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le titre se situe: en haut\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def detecter_position_titre(image_path):\n",
    "    # Charger l'image avec OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Appliquer un traitement d'image pour améliorer la lisibilité du texte (par exemple, seuillage adaptatif)\n",
    "    processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Utiliser Tesseract pour détecter les contours de texte\n",
    "    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Trouver le contour le plus grand (qui correspond probablement au titre)\n",
    "    contour_titre = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Calculer le rectangle englobant le contour du titre\n",
    "    x, y, w, h = cv2.boundingRect(contour_titre)\n",
    "\n",
    "    # Déterminer la position du titre en analysant la position verticale du rectangle englobant\n",
    "    hauteur, largeur = image.shape[:2]\n",
    "    position = \"\"\n",
    "    position_verticale = y / hauteur\n",
    "    if position_verticale < 0.3:\n",
    "        position = \"en haut\"\n",
    "    elif position_verticale > 0.7:\n",
    "        position = \"en bas\"\n",
    "    else:\n",
    "        position = \"au milieu\"\n",
    "\n",
    "    return position\n",
    "\n",
    "# Chemin vers l'image PNG contenant le poster de film\n",
    "chemin_image = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/vik.jpeg\"\n",
    "\n",
    "# Appeler la fonction pour détecter la position du titre\n",
    "position_titre = detecter_position_titre(chemin_image)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Le titre se situe:\", position_titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le titre se situe: en bas\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def detecter_position_titre(image_path):\n",
    "    # Charger l'image avec OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Appliquer un traitement d'image pour améliorer la lisibilité du texte (par exemple, seuillage adaptatif)\n",
    "    processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Utiliser Tesseract pour détecter les contours de texte\n",
    "    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Trier les contours par taille décroissante\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Sélectionner les deux contours les plus grands (probablement le titre et un autre texte)\n",
    "    premier_contour = contours[0]\n",
    "    deuxieme_contour = contours[1]\n",
    "\n",
    "    # Récupérer les coordonnées des rectangles englobants pour les deux contours\n",
    "    x1, y1, w1, h1 = cv2.boundingRect(premier_contour)\n",
    "    x2, y2, w2, h2 = cv2.boundingRect(deuxieme_contour)\n",
    "\n",
    "    # Calculer les positions verticales des deux rectangles englobants\n",
    "    hauteur, largeur = image.shape[:2]\n",
    "    position_titre = y1 / hauteur\n",
    "    position_autre = y2 / hauteur\n",
    "\n",
    "    # Déterminer la position du titre en comparant les positions verticales\n",
    "    position = \"\"\n",
    "    if position_titre < position_autre:\n",
    "        position = \"en haut\"\n",
    "    else:\n",
    "        position = \"en bas\"\n",
    "\n",
    "    return position\n",
    "\n",
    "# Chemin vers l'image PNG contenant le poster de film\n",
    "chemin_image = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/TheNun.jpg\"\n",
    "\n",
    "# Appeler la fonction pour détecter la position du titre\n",
    "position_titre = detecter_position_titre(chemin_image)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Le titre se situe:\", position_titre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6018e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TesseractError",
     "evalue": "(1, 'Error opening data file /opt/homebrew/share/tessdata/fra.traineddata Please make sure the TESSDATA_PREFIX environment variable is set to your \"tessdata\" directory. Failed loading language \\'fra\\' Tesseract couldn\\'t load any languages! Could not initialize tesseract.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTesseractError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m chemin_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/mer.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Appeler la fonction pour détecter la position du titre et récupérer les textes\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m position_titre, titre, autre_texte \u001b[38;5;241m=\u001b[39m \u001b[43mdetecter_position_titre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchemin_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Afficher le résultat\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe titre se situe:\u001b[39m\u001b[38;5;124m\"\u001b[39m, position_titre)\n",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m, in \u001b[0;36mdetecter_position_titre\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m x2, y2, w2, h2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mboundingRect(deuxieme_contour)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Détecter les textes dans les rectangles englobants\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m titre \u001b[38;5;241m=\u001b[39m \u001b[43mpytesseract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[43my1\u001b[49m\u001b[43m:\u001b[49m\u001b[43my1\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfra\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m autre_texte \u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_string(processed[y2:y2\u001b[38;5;241m+\u001b[39mh2, x2:x2\u001b[38;5;241m+\u001b[39mw2], lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculer les positions verticales des deux rectangles englobants\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/pytesseract/pytesseract.py:423\u001b[0m, in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBYTES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDICT\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTRING\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/pytesseract/pytesseract.py:426\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    421\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    424\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    425\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[0;32m--> 426\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_and_get_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    427\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/pytesseract/pytesseract.py:288\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[1;32m    278\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    286\u001b[0m     }\n\u001b[0;32m--> 288\u001b[0m     \u001b[43mrun_tesseract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m output_file:\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.9/site-packages/pytesseract/pytesseract.py:264\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractError(proc\u001b[38;5;241m.\u001b[39mreturncode, get_errors(error_string))\n",
      "\u001b[0;31mTesseractError\u001b[0m: (1, 'Error opening data file /opt/homebrew/share/tessdata/fra.traineddata Please make sure the TESSDATA_PREFIX environment variable is set to your \"tessdata\" directory. Failed loading language \\'fra\\' Tesseract couldn\\'t load any languages! Could not initialize tesseract.')"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def detecter_position_titre(image_path):\n",
    "    # Charger l'image avec OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Appliquer un traitement d'image pour améliorer la lisibilité du texte (par exemple, seuillage adaptatif)\n",
    "    processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Utiliser Tesseract pour détecter les contours de texte\n",
    "    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Trier les contours par taille décroissante\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "    # Sélectionner les deux contours les plus grands (probablement le titre et un autre texte)\n",
    "    premier_contour = contours[0]\n",
    "    deuxieme_contour = contours[1]\n",
    "\n",
    "    # Récupérer les coordonnées des rectangles englobants pour les deux contours\n",
    "    x1, y1, w1, h1 = cv2.boundingRect(premier_contour)\n",
    "    x2, y2, w2, h2 = cv2.boundingRect(deuxieme_contour)\n",
    "\n",
    "    # Détecter les textes dans les rectangles englobants\n",
    "    titre = pytesseract.image_to_string(processed[y1:y1+h1, x1:x1+w1], lang='fra')\n",
    "    autre_texte = pytesseract.image_to_string(processed[y2:y2+h2, x2:x2+w2], lang='fra')\n",
    "\n",
    "    # Calculer les positions verticales des deux rectangles englobants\n",
    "    hauteur, largeur = image.shape[:2]\n",
    "    position_titre = y1 / hauteur\n",
    "    position_autre = y2 / hauteur\n",
    "\n",
    "    # Déterminer la position du titre en comparant les positions verticales\n",
    "    position = \"\"\n",
    "    if position_titre < position_autre:\n",
    "        position = \"en haut\"\n",
    "    else:\n",
    "        position = \"en bas\"\n",
    "\n",
    "    return position, titre, autre_texte\n",
    "\n",
    "# Chemin vers l'image PNG contenant le poster de film\n",
    "chemin_image = r\"/Users/dimitriraymond/Documents/Epitech/Semestre 2/DAT/mer.jpeg\"\n",
    "\n",
    "# Appeler la fonction pour détecter la position du titre et récupérer les textes\n",
    "position_titre, titre, autre_texte = detecter_position_titre(chemin_image)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"Le titre se situe:\", position_titre)\n",
    "print(\"Titre:\", titre)\n",
    "print(\"Autre texte:\", autre_texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da42f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
